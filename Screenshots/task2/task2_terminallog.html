<pre>(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~</b></font>$ cd ~/cs6886w_a2
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ git clone https://huggingface.co/openai-community/gpt2-medium
Cloning into &apos;gpt2-medium&apos;...
remote: Enumerating objects: 76, done.
remote: Total 76 (delta 0), reused 0 (delta 0), pack-reused 76 (from 1)
Unpacking objects: 100% (76/76), 1.65 MiB | 3.40 MiB/s, done.
Filtering content: 100% (8/8), 11.63 GiB | 3.93 MiB/s, done.
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ls
<font color="#12488B"><b>gpt2-medium</b></font>  <font color="#12488B"><b>llama.cpp</b></font>
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ cd gpt2-medium/
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/gpt2-medium</b></font>$ ls
config.json         generation_config_for_text_generation.json  merges.txt         <font color="#12488B"><b>onnx</b></font>               README.md      tf_model.h5            tokenizer.json
flax_model.msgpack  generation_config.json                      model.safetensors  pytorch_model.bin  rust_model.ot  tokenizer_config.json  vocab.json
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/gpt2-medium</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/gpt2-medium</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/gpt2-medium</b></font>$ cd ..
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/convert_hf_to_gguf.py ./gpt2-medium --outfile ./gpt2-medium/gpt2-medium.gguf
Traceback (most recent call last):
  File &quot;/home/mahendranv/cs6886w_a2/./llama.cpp/convert_hf_to_gguf.py&quot;, line 19, in &lt;module&gt;
    from transformers import AutoConfig
ModuleNotFoundError: No module named &apos;transformers&apos;
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pip show transformers
bash: /home/mahendranv/cs6886w_a2/.venv/bin/pip: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pip install --upgrade pip
bash: /home/mahendranv/cs6886w_a2/.venv/bin/pip: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$  pip install transformers accelerate
bash: /home/mahendranv/cs6886w_a2/.venv/bin/pip: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pip install --break-system-packages transformers accelerate
bash: /home/mahendranv/cs6886w_a2/.venv/bin/pip: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ sudo apt update
[sudo] password for mahendranv: 
Hit:1 http://archive.ubuntu.com/ubuntu noble InRelease                                                                   
Get:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]                                  <font color="#A2734C">              </font>
Hit:3 https://apt.repos.intel.com/oneapi all InRelease                                                               
Err:3 https://apt.repos.intel.com/oneapi all InRelease                                          <font color="#A2734C">                     </font>
  The following signatures couldn&apos;t be verified because the public key is not available: NO_PUBKEY BAC6F0C353D04109
Get:4 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]                         
Err:5 https://repositories.intel.com/oneapi all InRelease<font color="#A2734C">                                                 </font>
  403  Forbidden [IP: 65.8.17.104 443]
Get:6 http://security.ubuntu.com/ubuntu noble-security/main amd64 Components [21.6 kB]
Get:7 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Components [212 B]
Get:8 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Components [52.2 kB]
Get:9 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]<font color="#A2734C">                      </font>
Get:10 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Components [212 B]
Get:11 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [1,585 kB]
Get:12 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Components [175 kB]
Get:13 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Components [212 B]
Get:14 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1,499 kB]
Get:15 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Components [378 kB]
Get:16 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Components [940 B]
Get:17 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Components [7,140 B]
Get:18 http://archive.ubuntu.com/ubuntu noble-backports/restricted amd64 Components [216 B]
Get:19 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Components [11.0 kB]
Get:20 http://archive.ubuntu.com/ubuntu noble-backports/multiverse amd64 Components [212 B]
Reading package lists... Done<font color="#A2734C">     </font>
<font color="#A2734C"><b>W: </b></font>An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://apt.repos.intel.com/oneapi all InRelease: The following signatures couldn&apos;t be verified because the public key is not available: NO_PUBKEY BAC6F0C353D04109
<font color="#C01C28"><b>E: </b></font>Failed to fetch https://repositories.intel.com/oneapi/dists/all/InRelease  403  Forbidden [IP: 65.8.17.104 443]
<font color="#C01C28"><b>E: </b></font>The repository &apos;https://repositories.intel.com/oneapi all InRelease&apos; is not signed.
<font color="#A2734C">N: </font>Updating from such a repository can&apos;t be done securely, and is therefore disabled by default.
<font color="#A2734C">N: </font>See apt-secure(8) manpage for repository creation and user configuration details.
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ sudo apt install python3-venv python3-full -y
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
python3-venv is already the newest version (3.12.3-0ubuntu2.1).
The following package was automatically installed and is no longer required:
  libllvm19
Use &apos;sudo apt autoremove&apos; to remove it.
The following additional packages will be installed:
  2to3 fonts-mathjax idle idle-python3.12 libjs-mathjax libpython3.12-testsuite net-tools python3-doc python3-examples python3-lib2to3 python3.12-doc python3.12-examples
  python3.12-full
Suggested packages:
  fonts-mathjax-extras fonts-stix libjs-mathjax-doc
The following NEW packages will be installed:
  2to3 fonts-mathjax idle idle-python3.12 libjs-mathjax libpython3.12-testsuite net-tools python3-doc python3-examples python3-full python3-lib2to3 python3.12-doc python3.12-examples
  python3.12-full
0 upgraded, 14 newly installed, 0 to remove and 6 not upgraded.
Need to get 26.1 MB of archives.
After this operation, 150 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu noble/universe amd64 python3-lib2to3 all 3.12.3-0ubuntu1 [78.0 kB]
Get:2 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 2to3 all 3.12.3-0ubuntu2.1 [11.1 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble/main amd64 fonts-mathjax all 2.7.9+dfsg-1 [2,208 kB]
Get:4 http://archive.ubuntu.com/ubuntu noble/main amd64 libjs-mathjax all 2.7.9+dfsg-1 [5,665 kB]
Get:5 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 idle-python3.12 all 3.12.3-1ubuntu0.8 [423 kB]
Get:6 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 idle all 3.12.3-0ubuntu2.1 [2,728 B]
Get:7 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 net-tools amd64 2.10-0.1ubuntu4.4 [204 kB]
Get:8 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 libpython3.12-testsuite all 3.12.3-1ubuntu0.8 [4,635 kB]
Get:9 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-doc all 3.12.3-1ubuntu0.8 [12.1 MB]                                                                         
Get:10 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3-doc all 3.12.3-0ubuntu2.1 [10.3 kB]                                                                           
Get:11 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3.12-examples all 3.12.3-1ubuntu0.8 [797 kB]                                                                    
Get:12 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 python3-examples all 3.12.3-0ubuntu2.1 [884 B]                                                                        
Get:13 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3.12-full amd64 3.12.3-1ubuntu0.8 [1,120 B]                                                                 
Get:14 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-full amd64 3.12.3-0ubuntu2.1 [1,156 B]                                                                    
Fetched 26.1 MB in 10s (2,591 kB/s)                                                                                                                                                    
Selecting previously unselected package python3-lib2to3.
(Reading database ... 193880 files and directories currently installed.)
Preparing to unpack .../00-python3-lib2to3_3.12.3-0ubuntu1_all.deb ...
Unpacking python3-lib2to3 (3.12.3-0ubuntu1) ...
Selecting previously unselected package 2to3.
Preparing to unpack .../01-2to3_3.12.3-0ubuntu2.1_all.deb ...
Unpacking 2to3 (3.12.3-0ubuntu2.1) ...
Selecting previously unselected package fonts-mathjax.
Preparing to unpack .../02-fonts-mathjax_2.7.9+dfsg-1_all.deb ...
Unpacking fonts-mathjax (2.7.9+dfsg-1) ...
Selecting previously unselected package libjs-mathjax.
Preparing to unpack .../03-libjs-mathjax_2.7.9+dfsg-1_all.deb ...
Unpacking libjs-mathjax (2.7.9+dfsg-1) ...
Selecting previously unselected package idle-python3.12.
Preparing to unpack .../04-idle-python3.12_3.12.3-1ubuntu0.8_all.deb ...
Unpacking idle-python3.12 (3.12.3-1ubuntu0.8) ...
Selecting previously unselected package idle.
Preparing to unpack .../05-idle_3.12.3-0ubuntu2.1_all.deb ...
Unpacking idle (3.12.3-0ubuntu2.1) ...
Selecting previously unselected package net-tools.
Preparing to unpack .../06-net-tools_2.10-0.1ubuntu4.4_amd64.deb ...
Unpacking net-tools (2.10-0.1ubuntu4.4) ...
Selecting previously unselected package libpython3.12-testsuite.
Preparing to unpack .../07-libpython3.12-testsuite_3.12.3-1ubuntu0.8_all.deb ...
Unpacking libpython3.12-testsuite (3.12.3-1ubuntu0.8) ...
Selecting previously unselected package python3.12-doc.
Preparing to unpack .../08-python3.12-doc_3.12.3-1ubuntu0.8_all.deb ...
Unpacking python3.12-doc (3.12.3-1ubuntu0.8) ...
Selecting previously unselected package python3-doc.
Preparing to unpack .../09-python3-doc_3.12.3-0ubuntu2.1_all.deb ...
Unpacking python3-doc (3.12.3-0ubuntu2.1) ...
Selecting previously unselected package python3.12-examples.
Preparing to unpack .../10-python3.12-examples_3.12.3-1ubuntu0.8_all.deb ...
Unpacking python3.12-examples (3.12.3-1ubuntu0.8) ...
Selecting previously unselected package python3-examples.
Preparing to unpack .../11-python3-examples_3.12.3-0ubuntu2.1_all.deb ...
Unpacking python3-examples (3.12.3-0ubuntu2.1) ...
Selecting previously unselected package python3.12-full.
Preparing to unpack .../12-python3.12-full_3.12.3-1ubuntu0.8_amd64.deb ...
Unpacking python3.12-full (3.12.3-1ubuntu0.8) ...
Selecting previously unselected package python3-full.
Preparing to unpack .../13-python3-full_3.12.3-0ubuntu2.1_amd64.deb ...
Unpacking python3-full (3.12.3-0ubuntu2.1) ...
Setting up net-tools (2.10-0.1ubuntu4.4) ...
Setting up fonts-mathjax (2.7.9+dfsg-1) ...
Setting up libjs-mathjax (2.7.9+dfsg-1) ...
Setting up python3.12-examples (3.12.3-1ubuntu0.8) ...
Setting up libpython3.12-testsuite (3.12.3-1ubuntu0.8) ...
Setting up python3.12-doc (3.12.3-1ubuntu0.8) ...
Setting up python3-lib2to3 (3.12.3-0ubuntu1) ...
Setting up python3-doc (3.12.3-0ubuntu2.1) ...
Setting up idle-python3.12 (3.12.3-1ubuntu0.8) ...
Setting up python3.12-full (3.12.3-1ubuntu0.8) ...
Setting up idle (3.12.3-0ubuntu2.1) ...
Setting up 2to3 (3.12.3-0ubuntu2.1) ...
Setting up python3-examples (3.12.3-0ubuntu2.1) ...
Setting up python3-full (3.12.3-0ubuntu2.1) ...
Processing triggers for install-info (7.1-3build2) ...
Processing triggers for fontconfig (2.15.0-1.1ubuntu2) ...
Processing triggers for desktop-file-utils (0.27-2build1) ...
Processing triggers for gnome-menus (3.36.0-1.1ubuntu3) ...
Processing triggers for man-db (2.12.0-4build2) ...
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ rm -rf .venv
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ python3 -m venv .venv
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ls .venv/bin
activate  activate.csh  activate.fish  Activate.ps1  <font color="#26A269"><b>pip</b></font>  <font color="#26A269"><b>pip3</b></font>  <font color="#26A269"><b>pip3.12</b></font>  <font color="#2AA1B3"><b>python</b></font>  <font color="#2AA1B3"><b>python3</b></font>  <font color="#2AA1B3"><b>python3.12</b></font>
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ source .venv/bin/activate
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pip install --upgrade pip
Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (24.0)
Collecting pip
  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)
Downloading pip-25.3-py3-none-any.whl (1.8 MB)
   <font color="#729C1F">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</font> <font color="#26A269">1.8/1.8 MB</font> <font color="#C01C28">4.1 MB/s</font> eta <font color="#2AA1B3">0:00:00</font>
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 24.0
    Uninstalling pip-24.0:
      Successfully uninstalled pip-24.0
Successfully installed pip-25.3
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pip install transformers accelerate
Collecting transformers
  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)
Collecting accelerate
  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)
Collecting filelock (from transformers)
  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)
Collecting huggingface-hub&lt;1.0,&gt;=0.34.0 (from transformers)
  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)
Collecting numpy&gt;=1.17 (from transformers)
  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)
Collecting packaging&gt;=20.0 (from transformers)
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting pyyaml&gt;=5.1 (from transformers)
  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting regex!=2019.12.17 (from transformers)
  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting requests (from transformers)
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting tokenizers&lt;=0.23.0,&gt;=0.22.0 (from transformers)
  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting safetensors&gt;=0.4.3 (from transformers)
  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting tqdm&gt;=4.27 (from transformers)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting fsspec&gt;=2023.5.0 (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers)
  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)
Collecting typing-extensions&gt;=3.7.4.3 (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers)
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting hf-xet&lt;2.0.0,&gt;=1.1.3 (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers)
  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)
Collecting psutil (from accelerate)
  Using cached psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)
Collecting torch&gt;=2.0.0 (from accelerate)
  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)
Collecting setuptools (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)
Collecting sympy&gt;=1.13.3 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx&gt;=2.5.1 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cublas-cu12==12.8.4.1 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufft-cu12==11.3.3.83 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-curand-cu12==10.3.9.90 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparselt-cu12==0.7.1 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)
Collecting nvidia-nccl-cu12==2.27.5 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)
Collecting nvidia-nvshmem-cu12==3.3.20 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)
Collecting nvidia-nvtx-cu12==12.8.90 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufile-cu12==1.13.1.3 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.5.1 (from torch&gt;=2.0.0-&gt;accelerate)
  Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)
Collecting mpmath&lt;1.4,&gt;=1.1.0 (from sympy&gt;=1.13.3-&gt;torch&gt;=2.0.0-&gt;accelerate)
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting MarkupSafe&gt;=2.0 (from jinja2-&gt;torch&gt;=2.0.0-&gt;accelerate)
  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting charset_normalizer&lt;4,&gt;=2 (from requests-&gt;transformers)
  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting idna&lt;4,&gt;=2.5 (from requests-&gt;transformers)
  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting urllib3&lt;3,&gt;=1.21.1 (from requests-&gt;transformers)
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi&gt;=2017.4.17 (from requests-&gt;transformers)
  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)
Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)
Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)
Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)
Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)
Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)
Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)
Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)
Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)
Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)
Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)
Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)
Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)
Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)
Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)
Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)
Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)
Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)
Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)
Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)
Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)
Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)
Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)
Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)
Using cached networkx-3.5-py3-none-any.whl (2.0 MB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached filelock-3.20.0-py3-none-any.whl (16 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Using cached psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)
Using cached requests-2.32.5-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)
Using cached idna-3.11-py3-none-any.whl (71 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)
Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)
Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, triton, tqdm, sympy, setuptools, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate
Successfully installed MarkupSafe-3.0.3 accelerate-1.11.0 certifi-2025.11.12 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 packaging-25.0 psutil-7.1.3 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.6.2 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.1 triton-3.5.1 typing-extensions-4.15.0 urllib3-2.5.0
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pip show transformers
Name: transformers
Version: 4.57.1
Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow
Home-page: https://github.com/huggingface/transformers
Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)
Author-email: transformers@huggingface.co
License: Apache 2.0 License
Location: /home/mahendranv/cs6886w_a2/.venv/lib/python3.12/site-packages
Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm
Required-by: 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ pwd
/home/mahendranv/cs6886w_a2
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/convert_hf_to_gguf.py ./gpt2-medium --outfile ./gpt2-medium/gpt2-medium.gguf
INFO:hf-to-gguf:Loading model: gpt2-medium
INFO:hf-to-gguf:Model architecture: GPT2LMHeadModel
INFO:hf-to-gguf:gguf: indexing model part &apos;model.safetensors&apos;
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.12.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.12.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.12.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.13.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.13.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.13.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.14.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.14.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.14.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.15.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.15.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.15.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.16.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.16.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.16.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.17.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.17.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.17.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.18.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.18.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.18.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.19.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.19.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.19.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.20.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.20.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.20.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.21.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.21.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.21.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.22.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.22.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.22.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.23.attn_qkv.bias,      torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.23.attn_output.bias,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_norm.bias,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.ffn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.ffn_up.bias,        torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.23.ffn_down.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float32 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float32 --&gt; F32, shape = {4096}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --&gt; F16, shape = {1024, 4096}
INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --&gt; F16, shape = {4096, 1024}
INFO:hf-to-gguf:output_norm.bias,          torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:output_norm.weight,        torch.float32 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:position_embd.weight,      torch.float32 --&gt; F32, shape = {1024, 1024}
INFO:hf-to-gguf:token_embd.weight,         torch.float32 --&gt; F16, shape = {1024, 50257}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:Set model quantization version
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 50000 merge(s).
INFO:gguf.vocab:Setting special token type bos to 50256
INFO:gguf.vocab:Setting special token type eos to 50256
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:gpt2-medium/gpt2-medium.gguf: n_tensors = 292, total_size = 712.4M
Writing:   0%|                                                                                                                                            | 0.00/712M [00:00&lt;?, ?byte/s]/home/mahendranv/cs6886w_a2/llama.cpp/gguf-py/gguf/lazy.py:222: RuntimeWarning: overflow encountered in cast
  return type(self)(meta=meta, args=full_args, kwargs=kwargs, func=(lambda a, *args, **kwargs: a.astype(*args, **kwargs)))
Writing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712M/712M [00:25&lt;00:00, 28.3Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to gpt2-medium/gpt2-medium.gguf
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ task2_step2_after_conversion_ls_02
task2_step2_after_conversion_ls_02: command not found
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ mkdir -p ~/cs6886w_a2/perf_logs/task2/
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench -m ./gpt2-medium/gpt2-medium.gguf -p 0 -n 256  | tee ~/cs6886w_a2/perf_logs/task2/task2_sanity_benchmark.txt
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           tg256 |         50.40 ± 1.45 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ls 
<font color="#12488B"><b>gpt2-medium</b></font>  <font color="#12488B"><b>llama.cpp</b></font>  <font color="#12488B"><b>perf_logs</b></font>
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ls perf_logs/task2/task2_sanity_benchmark.txt 
perf_logs/task2/task2_sanity_benchmark.txt
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ls -lrt
total 12
drwxrwxr-x 26 mahendranv mahendranv 4096 Nov 17 16:46 <font color="#12488B"><b>llama.cpp</b></font>
drwxrwxr-x  4 mahendranv mahendranv 4096 Nov 17 18:46 <font color="#12488B"><b>gpt2-medium</b></font>
drwxrwxr-x  3 mahendranv mahendranv 4096 Nov 17 18:50 <font color="#12488B"><b>perf_logs</b></font>
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ cd llama.cpp/
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ ls -lrt
total 940
-rw-rw-r--  1 mahendranv mahendranv   5347 Nov 17 16:44 SECURITY.md
-rw-rw-r--  1 mahendranv mahendranv  30741 Nov 17 16:44 README.md
-rw-rw-r--  1 mahendranv mahendranv    257 Nov 17 16:44 Makefile
-rw-rw-r--  1 mahendranv mahendranv   1078 Nov 17 16:44 LICENSE
-rw-rw-r--  1 mahendranv mahendranv   8481 Nov 17 16:44 CONTRIBUTING.md
-rw-rw-r--  1 mahendranv mahendranv   6143 Nov 17 16:44 CODEOWNERS
-rw-rw-r--  1 mahendranv mahendranv   4570 Nov 17 16:44 CMakePresets.json
-rw-rw-r--  1 mahendranv mahendranv   8514 Nov 17 16:44 CMakeLists.txt
-rw-rw-r--  1 mahendranv mahendranv  47860 Nov 17 16:44 AUTHORS
drwxrwxr-x  3 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>benches</b></font>
-rwxrwxr-x  1 mahendranv mahendranv  21904 Nov 17 16:44 <font color="#26A269"><b>build-xcframework.sh</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>ci</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>cmake</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>common</b></font>
-rwxrwxr-x  1 mahendranv mahendranv  19106 Nov 17 16:44 <font color="#26A269"><b>convert_llama_ggml_to_gguf.py</b></font>
-rwxrwxr-x  1 mahendranv mahendranv  24610 Nov 17 16:44 <font color="#26A269"><b>convert_hf_to_gguf_update.py</b></font>
-rwxrwxr-x  1 mahendranv mahendranv 486306 Nov 17 16:44 <font color="#26A269"><b>convert_hf_to_gguf.py</b></font>
-rwxrwxr-x  1 mahendranv mahendranv  20291 Nov 17 16:44 <font color="#26A269"><b>convert_lora_to_gguf.py</b></font>
drwxrwxr-x  6 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>docs</b></font>
-rw-rw-r--  1 mahendranv mahendranv   7243 Nov 17 16:44 flake.nix
-rw-rw-r--  1 mahendranv mahendranv   1556 Nov 17 16:44 flake.lock
drwxrwxr-x 28 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>examples</b></font>
drwxrwxr-x  5 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>ggml</b></font>
drwxrwxr-x  5 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>gguf-py</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>grammars</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>include</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>licenses</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>media</b></font>
drwxrwxr-x  3 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>models</b></font>
-rw-rw-r--  1 mahendranv mahendranv    163 Nov 17 16:44 mypy.ini
drwxrwxr-x  3 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>pocs</b></font>
-rw-rw-r--  1 mahendranv mahendranv    551 Nov 17 16:44 requirements.txt
-rw-rw-r--  1 mahendranv mahendranv    616 Nov 17 16:44 pyrightconfig.json
-rw-rw-r--  1 mahendranv mahendranv   1336 Nov 17 16:44 pyproject.toml
-rw-rw-r--  1 mahendranv mahendranv 124786 Nov 17 16:44 poetry.lock
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>requirements</b></font>
drwxrwxr-x  5 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>scripts</b></font>
drwxrwxr-x  3 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>src</b></font>
drwxrwxr-x  2 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>tests</b></font>
drwxrwxr-x 17 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>tools</b></font>
drwxrwxr-x  7 mahendranv mahendranv   4096 Nov 17 16:44 <font color="#12488B"><b>vendor</b></font>
drwxrwxr-x 13 mahendranv mahendranv   4096 Nov 17 16:46 <font color="#12488B"><b>build</b></font>
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ ./llama.cpp/build/bin/llama-bench --help
bash: ./llama.cpp/build/bin/llama-bench: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ cd ..
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench --help
usage: ./llama.cpp/build/bin/llama-bench [options]

options:
  -h, --help
  --numa &lt;distribute|isolate|numactl&gt;       numa mode (default: disabled)
  -r, --repetitions &lt;n&gt;                     number of times to repeat each test (default: 5)
  --prio &lt;-1|0|1|2|3&gt;                          process/thread priority (default: 0)
  --delay &lt;0...N&gt; (seconds)                 delay between each test (default: 0)
  -o, --output &lt;csv|json|jsonl|md|sql&gt;      output format printed to stdout (default: md)
  -oe, --output-err &lt;csv|json|jsonl|md|sql&gt; output format printed to stderr (default: none)
  --list-devices                            list available devices and exit
  -v, --verbose                             verbose output
  --progress                                print test progress indicators
  --no-warmup                               skip warmup runs before benchmarking

test parameters:
  -m, --model &lt;filename&gt;                    (default: models/7B/ggml-model-q4_0.gguf)
  -p, --n-prompt &lt;n&gt;                        (default: 512)
  -n, --n-gen &lt;n&gt;                           (default: 128)
  -pg &lt;pp,tg&gt;                               (default: )
  -d, --n-depth &lt;n&gt;                         (default: 0)
  -b, --batch-size &lt;n&gt;                      (default: 2048)
  -ub, --ubatch-size &lt;n&gt;                    (default: 512)
  -ctk, --cache-type-k &lt;t&gt;                  (default: f16)
  -ctv, --cache-type-v &lt;t&gt;                  (default: f16)
  -t, --threads &lt;n&gt;                         (default: 4)
  -C, --cpu-mask &lt;hex,hex&gt;                  (default: 0x0)
  --cpu-strict &lt;0|1&gt;                        (default: 0)
  --poll &lt;0...100&gt;                          (default: 50)
  -ngl, --n-gpu-layers &lt;n&gt;                  (default: 99)
  -ncmoe, --n-cpu-moe &lt;n&gt;                   (default: 0)
  -sm, --split-mode &lt;none|layer|row&gt;        (default: layer)
  -mg, --main-gpu &lt;i&gt;                       (default: 0)
  -nkvo, --no-kv-offload &lt;0|1&gt;              (default: 0)
  -fa, --flash-attn &lt;0|1&gt;                   (default: 0)
  -dev, --device &lt;dev0/dev1/...&gt;            (default: auto)
  -mmp, --mmap &lt;0|1&gt;                        (default: 1)
  -embd, --embeddings &lt;0|1&gt;                 (default: 0)
  -ts, --tensor-split &lt;ts0/ts1/..&gt;          (default: 0)
  -ot --override-tensor &lt;tensor name pattern&gt;=&lt;buffer type&gt;;...
                                            (default: disabled)
  -nopo, --no-op-offload &lt;0|1&gt;              (default: 0)
  --no-host &lt;0|1&gt;                           (default: 0)

Multiple values can be given for each parameter by separating them with &apos;,&apos;
or by specifying the parameter multiple times. Ranges can be given as
&apos;first-last&apos; or &apos;first-last+step&apos; or &apos;first-last*mult&apos;.
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
  -m gpt2-medium/gpt2-medium.gguf \
  -p 0 -n 256
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           tg256 |         48.51 ± 1.42 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama-bench -m gpt2-medium.gguf -p 0 -n 256 -t 1
bash: ./llama-bench: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ cd llama.cpp/
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ ./llama-bench -m gpt2-medium.gguf -p 0 -n 256 -t 1
bash: ./llama-bench: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -p 0 -n 256 -t 1
bash: ./llama.cpp/build/bin/llama-bench: No such file or directory
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2/llama.cpp</b></font>$ cd ..
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -p 0 -n 256 -t 1
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       1 |           tg256 |         24.95 ± 0.63 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -p 0 -n 512 -t 4
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           tg512 |         47.15 ± 0.58 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -n 256 -t 4 -r 10
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           pp512 |       292.68 ± 54.55 |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           tg256 |         48.44 ± 0.34 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -n 256 -o json &gt; task4_output.json
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ cat task4_output.json
[
  {
    &quot;build_commit&quot;: &quot;2376b7758&quot;,
    &quot;build_number&quot;: 7083,
    &quot;cpu_info&quot;: &quot;11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz&quot;,
    &quot;gpu_info&quot;: &quot;&quot;,
    &quot;backends&quot;: &quot;CPU&quot;,
    &quot;model_filename&quot;: &quot;./gpt2-medium/gpt2-medium.gguf&quot;,
    &quot;model_type&quot;: &quot;gpt2 0.4B F16&quot;,
    &quot;model_size&quot;: 712386560,
    &quot;model_n_params&quot;: 354823168,
    &quot;n_batch&quot;: 2048,
    &quot;n_ubatch&quot;: 512,
    &quot;n_threads&quot;: 4,
    &quot;cpu_mask&quot;: &quot;0x0&quot;,
    &quot;cpu_strict&quot;: false,
    &quot;poll&quot;: 50,
    &quot;type_k&quot;: &quot;f16&quot;,
    &quot;type_v&quot;: &quot;f16&quot;,
    &quot;n_gpu_layers&quot;: 99,
    &quot;n_cpu_moe&quot;: 0,
    &quot;split_mode&quot;: &quot;layer&quot;,
    &quot;main_gpu&quot;: 0,
    &quot;no_kv_offload&quot;: false,
    &quot;flash_attn&quot;: false,
    &quot;devices&quot;: &quot;auto&quot;,
    &quot;tensor_split&quot;: &quot;0.00&quot;,
    &quot;tensor_buft_overrides&quot;: &quot;none&quot;,
    &quot;use_mmap&quot;: true,
    &quot;embeddings&quot;: false,
    &quot;no_op_offload&quot;: 0,
    &quot;no_host&quot;: false,
    &quot;n_prompt&quot;: 512,
    &quot;n_gen&quot;: 0,
    &quot;n_depth&quot;: 0,
    &quot;test_time&quot;: &quot;2025-11-17T13:58:59Z&quot;,
    &quot;avg_ns&quot;: 1483430630,
    &quot;stddev_ns&quot;: 158360609,
    &quot;avg_ts&quot;: 347.937904,
    &quot;stddev_ts&quot;: 32.708459,
    &quot;samples_ns&quot;: [ 1389869597, 1398983092, 1419824215, 1444249686, 1764226560 ],
    &quot;samples_ts&quot;: [ 368.38, 365.98, 360.608, 354.509, 290.212 ]
  },
  {
    &quot;build_commit&quot;: &quot;2376b7758&quot;,
    &quot;build_number&quot;: 7083,
    &quot;cpu_info&quot;: &quot;11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz&quot;,
    &quot;gpu_info&quot;: &quot;&quot;,
    &quot;backends&quot;: &quot;CPU&quot;,
    &quot;model_filename&quot;: &quot;./gpt2-medium/gpt2-medium.gguf&quot;,
    &quot;model_type&quot;: &quot;gpt2 0.4B F16&quot;,
    &quot;model_size&quot;: 712386560,
    &quot;model_n_params&quot;: 354823168,
    &quot;n_batch&quot;: 2048,
    &quot;n_ubatch&quot;: 512,
    &quot;n_threads&quot;: 4,
    &quot;cpu_mask&quot;: &quot;0x0&quot;,
    &quot;cpu_strict&quot;: false,
    &quot;poll&quot;: 50,
    &quot;type_k&quot;: &quot;f16&quot;,
    &quot;type_v&quot;: &quot;f16&quot;,
    &quot;n_gpu_layers&quot;: 99,
    &quot;n_cpu_moe&quot;: 0,
    &quot;split_mode&quot;: &quot;layer&quot;,
    &quot;main_gpu&quot;: 0,
    &quot;no_kv_offload&quot;: false,
    &quot;flash_attn&quot;: false,
    &quot;devices&quot;: &quot;auto&quot;,
    &quot;tensor_split&quot;: &quot;0.00&quot;,
    &quot;tensor_buft_overrides&quot;: &quot;none&quot;,
    &quot;use_mmap&quot;: true,
    &quot;embeddings&quot;: false,
    &quot;no_op_offload&quot;: 0,
    &quot;no_host&quot;: false,
    &quot;n_prompt&quot;: 0,
    &quot;n_gen&quot;: 256,
    &quot;n_depth&quot;: 0,
    &quot;test_time&quot;: &quot;2025-11-17T13:59:08Z&quot;,
    &quot;avg_ns&quot;: 5268611350,
    &quot;stddev_ns&quot;: 23927782,
    &quot;avg_ts&quot;: 48.590452,
    &quot;stddev_ts&quot;: 0.219738,
    &quot;samples_ns&quot;: [ 5308189109, 5274418991, 5253742557, 5254638388, 5252067706 ],
    &quot;samples_ts&quot;: [ 48.2274, 48.5362, 48.7272, 48.7189, 48.7427 ]
  }
]
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench --list-devices
Available devices:
  (none)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -n 256 --no-warmup
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           pp512 |       366.15 ± 13.55 |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           tg256 |         47.70 ± 1.16 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -n 256 -b 4096
| model                          |       size |     params | backend    | threads | n_batch |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |    4096 |           pp512 |       326.52 ± 45.40 |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |    4096 |           tg256 |         47.10 ± 1.59 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ sudo perf stat -e \
fp_arith_inst_retired.scalar_single,LLC-load-misses \
    ./llama.cpp/build/bin/llama-bench \
        -m ./gpt2-medium/gpt2-medium.gguf \
        -p 0 -n 256 -t 1
[sudo] password for mahendranv: 
| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       1 |           tg256 |         25.28 ± 0.90 |

build: 2376b7758 (7083)

 Performance counter stats for &apos;./llama.cpp/build/bin/llama-bench -m ./gpt2-medium/gpt2-medium.gguf -p 0 -n 256 -t 1&apos;:

       645,604,148      fp_arith_inst_retired.scalar_single                                      
     4,732,058,272      LLC-load-misses                                                       

      50.853230061 seconds time elapsed

      50.745511000 seconds user
       0.072979000 seconds sys


(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ mkdir -p perf_logs/task_demo
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ ./llama.cpp/build/bin/llama-bench \
    -m ./gpt2-medium/gpt2-medium.gguf \
    -p 0 -n 256 \
    | tee perf_logs/task_demo/demo_output.txt

| model                          |       size |     params | backend    | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |
| gpt2 0.4B F16                  | 679.38 MiB |   354.82 M | CPU        |       4 |           tg256 |         50.98 ± 1.09 |

build: 2376b7758 (7083)
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 
(.venv) <font color="#26A269"><b>mahendranv@mahendranv</b></font>:<font color="#12488B"><b>~/cs6886w_a2</b></font>$ 

</pre>